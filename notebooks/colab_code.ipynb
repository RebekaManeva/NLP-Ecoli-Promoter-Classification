{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring NLP Approaches for Classifying Promoter Regions in DNA Sequences\n",
        "\n",
        "Methodology in Information and Communication Technologies\n"
      ],
      "metadata": {
        "id": "lASDLIb4hIau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authors: Rebeka Maneva, Konstantin Lozankoski"
      ],
      "metadata": {
        "id": "0kVGYGjshcaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necessary installations"
      ],
      "metadata": {
        "id": "0MIwW3MddBMN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj6lR6xLphdb"
      },
      "outputs": [],
      "source": [
        "!pip install -q biopython scikit-learn tensorflow matplotlib seaborn tqdm xgboost transformers accelerate datasets pyarrow huggingface_hub\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np, pandas as pd, re, random, tensorflow as tf\n",
        "from random import shuffle\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from Bio import SeqIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import xgboost as xgb\n",
        "import torch\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, roc_curve,\n",
        "    precision_recall_curve, average_precision_score, classification_report\n",
        ")\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "login(userdata.get('HUGGINGFACE_TOKEN'))\n",
        "np.random.seed(42); tf.random.set_seed(42); random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search for \"!!!\" in the blocks to update the paths before running"
      ],
      "metadata": {
        "id": "44nbA0FGui1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA"
      ],
      "metadata": {
        "id": "uHke1SechnE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RegulonDB Promoter Dataset\n",
        "\n",
        "https://regulondb.ccg.unam.mx/datasets"
      ],
      "metadata": {
        "id": "mWoQih_XdQ0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the promotor sequences - generating promoter (positive) examples"
      ],
      "metadata": {
        "id": "nTk32-exdSwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "promoter_path = \"x/promoter.tsv\" # update this path !!!\n",
        "\n",
        "# key columns\n",
        "promoter_df = pd.read_csv(promoter_path, sep=\"\\t\", skiprows=33)\n",
        "seq_col = [c for c in promoter_df.columns if \"seq\" in c.lower()][0]\n",
        "pos_col = [c for c in promoter_df.columns if \"pos\" in c.lower()][0]\n",
        "\n",
        "# cleaning sequence and position\n",
        "promoter_df = promoter_df[[seq_col, pos_col]].dropna()\n",
        "promoter_df[seq_col] = promoter_df[seq_col].apply(lambda s: re.sub(r'[^ATCG]', '', str(s).upper()))\n",
        "promoter_df[pos_col] = promoter_df[pos_col].astype(int)\n",
        "promoter_df = promoter_df.rename(columns={seq_col: \"sequence\", pos_col: \"posTSS\"})\n",
        "promoter_df[\"label\"] = 1"
      ],
      "metadata": {
        "id": "R0l_JzK3plWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting sequences not containing promoters - generating nonpromoter (negative) examples"
      ],
      "metadata": {
        "id": "9ZUdyy_vdbC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gene_path = \"x/gene.tsv\" # update this path !!!\n",
        "\n",
        "gene_df = pd.read_csv(gene_path, sep=\"\\t\", skiprows=29)\n",
        "gene_seq_col = [c for c in gene_df.columns if \"seq\" in c.lower()][0]\n",
        "gene_id_col = [c for c in gene_df.columns if \"id\" in c.lower()][0]\n",
        "\n",
        "# cleaning up and preparing\n",
        "gene_df = gene_df[[gene_id_col, gene_seq_col]].dropna(subset=[gene_seq_col])\n",
        "gene_df[gene_seq_col] = gene_df[gene_seq_col].apply(lambda s: re.sub(r'[^ATCG]', '', str(s).upper()))\n",
        "gene_df = gene_df.rename(columns={gene_seq_col: \"sequence\"})\n",
        "gene_df[\"label\"] = 0\n",
        "\n",
        "\n",
        "# matching sizes to balance positives and negatives\n",
        "neg_df = gene_df.sample(n=len(promoter_df), random_state=42)\n",
        "balanced_df = pd.concat([promoter_df[[\"sequence\", \"label\"]], neg_df[[\"sequence\", \"label\"]]], ignore_index=True)\n",
        "balanced_df.drop_duplicates(subset=[\"sequence\"], inplace=True)"
      ],
      "metadata": {
        "id": "KYa3k0WipnZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting negatives from the genome itself"
      ],
      "metadata": {
        "id": "-uOeO6fIdequ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW = 100\n",
        "MIN_DISTANCE = 1000\n",
        "NEG_PER_POS = 1\n",
        "FASTA_PATH = \"x/sequence.fasta\" # update this path !!!\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# load genome\n",
        "record = SeqIO.read(FASTA_PATH, \"fasta\")\n",
        "genome = str(record.seq).upper()\n",
        "genome_len = len(genome)\n",
        "\n",
        "# known promoter centers (TSS positions)\n",
        "known_tss = promoter_df[\"posTSS\"].dropna().astype(int).tolist()\n",
        "known_tss = [p for p in known_tss if 0 <= p < genome_len]\n",
        "\n",
        "def extract_centered_window(genome_str, center_pos, window):\n",
        "    half = window // 2\n",
        "    start = center_pos - half\n",
        "    end = start + window\n",
        "    if start < 0 or end > len(genome_str):\n",
        "        return None\n",
        "    seq = genome_str[start:end]\n",
        "    if set(seq) <= {\"A\", \"T\", \"C\", \"G\"}:\n",
        "        return seq\n",
        "    return None\n",
        "\n",
        "def sample_genome_negatives(genome_str, known_centers, n_samples, window, min_distance):\n",
        "    centers = np.array(sorted(set(known_centers)), dtype=np.int64)\n",
        "    neg_seqs = []\n",
        "    neg_centers = []\n",
        "\n",
        "    def far_enough(pos):\n",
        "        idx = np.searchsorted(centers, pos)\n",
        "        candidates = []\n",
        "        if idx > 0:\n",
        "            candidates.append(abs(pos - centers[idx-1]))\n",
        "        if idx < len(centers):\n",
        "            candidates.append(abs(pos - centers[idx]))\n",
        "        if not candidates:\n",
        "            return True\n",
        "        return min(candidates) >= min_distance\n",
        "\n",
        "    attempts = 0\n",
        "    max_attempts = n_samples * 200\n",
        "    while len(neg_seqs) < n_samples and attempts < max_attempts:\n",
        "        attempts += 1\n",
        "        pos = random.randint(window//2, len(genome_str) - window//2 - 1)\n",
        "        if not far_enough(pos):\n",
        "            continue\n",
        "        seq = extract_centered_window(genome_str, pos, window)\n",
        "        if seq is None:\n",
        "            continue\n",
        "        neg_seqs.append(seq)\n",
        "        neg_centers.append(pos)\n",
        "\n",
        "    return neg_centers, neg_seqs\n",
        "\n",
        "# extract positive sequences\n",
        "pos_seqs = []\n",
        "pos_centers = []\n",
        "for tss in known_tss:\n",
        "    seq = extract_centered_window(genome, tss, WINDOW)\n",
        "    if seq is not None:\n",
        "        pos_seqs.append(seq)\n",
        "        pos_centers.append(tss)\n",
        "\n",
        "pos_df_genome = pd.DataFrame({\n",
        "    \"sequence\": pos_seqs,\n",
        "    \"label\": 1,\n",
        "    \"center_pos\": pos_centers\n",
        "})\n",
        "\n",
        "# sample negatives\n",
        "n_neg = int(len(pos_df_genome) * NEG_PER_POS)\n",
        "neg_centers, neg_seqs = sample_genome_negatives(\n",
        "    genome, pos_df_genome[\"center_pos\"].tolist(), n_neg, WINDOW, MIN_DISTANCE\n",
        ")\n",
        "\n",
        "neg_df_genome = pd.DataFrame({\n",
        "    \"sequence\": neg_seqs,\n",
        "    \"label\": 0,\n",
        "    \"center_pos\": neg_centers\n",
        "})"
      ],
      "metadata": {
        "id": "oW_3cUJ0pqYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting hard negatives which are just shuffled promoters - generating fake negative examples and Slightly mutating real promoter sequences (2 % of bases) - adding diversity, combating overfitting"
      ],
      "metadata": {
        "id": "OoquPtshdm6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffling bases but still preserving global GC content\n",
        "def gc_shuffled(seq):\n",
        "    bases = list(seq)\n",
        "    shuffle(bases)\n",
        "    return \"\".join(bases)\n",
        "\n",
        "# mutating 2% of bases\n",
        "def mutate(seq, rate=0.02):\n",
        "    bases = ['A','T','G','C']\n",
        "    s = list(seq)\n",
        "    for i in range(len(s)):\n",
        "        if random.random() < rate:\n",
        "            s[i] = random.choice(bases)\n",
        "    return \"\".join(s)\n",
        "\n",
        "# generating augmentations\n",
        "fake_negatives = [gc_shuffled(s) for s in promoter_df[\"sequence\"]]\n",
        "mutated_promoters = [mutate(s, 0.02) for s in promoter_df[\"sequence\"]]\n",
        "\n",
        "neg_df2 = pd.DataFrame({\"sequence\": fake_negatives, \"label\": 0})\n",
        "pos_df2 = pd.DataFrame({\"sequence\": mutated_promoters, \"label\": 1})\n",
        "\n",
        "# combining all\n",
        "balanced_df = pd.concat([\n",
        "    balanced_df,\n",
        "    neg_df2,\n",
        "    pos_df2,\n",
        "    neg_df_genome,\n",
        "    pos_df_genome\n",
        "], ignore_index=True).drop_duplicates(subset=[\"sequence\"])\n",
        "\n",
        "# matching lenghts\n",
        "def one_hot_encode(seq, maxlen=100):\n",
        "    mapping = {'A':[1,0,0,0],'T':[0,1,0,0],'G':[0,0,1,0],'C':[0,0,0,1],'N':[0,0,0,0]}\n",
        "    seq = seq.upper()[:maxlen].ljust(maxlen, 'N')\n",
        "    return np.array([mapping.get(base,[0,0,0,0]) for base in seq])\n",
        "\n",
        "maxlen = 100\n",
        "X = np.array([one_hot_encode(s, maxlen) for s in balanced_df.sequence])\n",
        "y = balanced_df.label.values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "V2c4jWiYpsCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING AND EVALUATION"
      ],
      "metadata": {
        "id": "UWI4-fUBd3Jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_X_y_from_df(df, window):\n",
        "    X = np.array([one_hot_encode(s, window) for s in df[\"sequence\"].values])\n",
        "    y = df[\"label\"].values.astype(int)\n",
        "    return X, y\n",
        "\n",
        "def build_cnn(window):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(window, 4)),\n",
        "        tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling1D(2),\n",
        "        tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling1D(2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_bilstm(window):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(window, 4)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_cnn_bilstm(window):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(window, 4)),\n",
        "        tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling1D(2),\n",
        "        tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling1D(2),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def eval_probs(y_true, y_prob, threshold=0.5):\n",
        "    y_pred = (y_prob >= threshold).astype(int)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"F1\": f1_score(y_true, y_pred, zero_division=0),\n",
        "        \"ROC_AUC\": roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan,\n",
        "        \"PR_AUC\": average_precision_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_roc_pr_curves(results_probs, title_prefix=\"\"):\n",
        "    plt.figure(figsize=(6,5))\n",
        "    for r in results_probs:\n",
        "        fpr, tpr, _ = roc_curve(r[\"y_true\"], r[\"y_prob\"])\n",
        "        auc = roc_auc_score(r[\"y_true\"], r[\"y_prob\"])\n",
        "        plt.plot(fpr, tpr, label=f'{r[\"name\"]} (AUC={auc:.3f})')\n",
        "    plt.plot([0,1], [0,1], linestyle='--')\n",
        "    plt.title(f\"{title_prefix} ROC Curves\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    for r in results_probs:\n",
        "        p, rec, _ = precision_recall_curve(r[\"y_true\"], r[\"y_prob\"])\n",
        "        ap = average_precision_score(r[\"y_true\"], r[\"y_prob\"])\n",
        "        plt.plot(rec, p, label=f'{r[\"name\"]} (AP={ap:.3f})')\n",
        "    plt.title(f\"{title_prefix} Precision–Recall Curves\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion(y_true, y_prob, name, threshold=0.5):\n",
        "    y_pred = (y_prob >= threshold).astype(int)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(f\"Confusion Matrix — {name} (thr={threshold})\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xnMrVwmHpwMZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All non-DNABERT models for window=100 and window=200 (Table A)"
      ],
      "metadata": {
        "id": "5Cd11d45eD-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZES = [100, 200]\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "all_results_A = []\n",
        "all_probs_for_plots = {}\n",
        "\n",
        "for window in WINDOW_SIZES:\n",
        "    Xw, yw = make_X_y_from_df(balanced_df, window)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        Xw, yw, test_size=0.2, stratify=yw, random_state=42\n",
        "    )\n",
        "\n",
        "    # XGBoost baseline\n",
        "    X_train_flat = X_train.reshape(len(X_train), -1)\n",
        "    X_test_flat  = X_test.reshape(len(X_test), -1)\n",
        "\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        eval_metric='logloss',\n",
        "        n_estimators=300,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9\n",
        "    )\n",
        "    xgb_model.fit(X_train_flat, y_train)\n",
        "    xgb_prob = xgb_model.predict_proba(X_test_flat)[:,1]\n",
        "\n",
        "    m = eval_probs(y_test, xgb_prob, threshold=0.5)\n",
        "    all_results_A.append({\"Window\": window, \"Model\":\"XGBoost\", **m})\n",
        "    all_probs_for_plots.setdefault(window, []).append({\"name\":\"XGBoost\", \"y_true\":y_test, \"y_prob\":xgb_prob})\n",
        "\n",
        "    # Deep models\n",
        "    deep_specs = [\n",
        "        (\"CNN\", build_cnn),\n",
        "        (\"BiLSTM\", build_bilstm),\n",
        "        (\"CNN+BiLSTM\", build_cnn_bilstm)\n",
        "    ]\n",
        "\n",
        "    for name, builder in deep_specs:\n",
        "        model = builder(window)\n",
        "\n",
        "        es = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_split=0.2,\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            callbacks=[es],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # probs + metrics\n",
        "        y_prob = model.predict(X_test, batch_size=512, verbose=0).ravel()\n",
        "        metrics = eval_probs(y_test, y_prob, threshold=0.5)\n",
        "        all_results_A.append({\"Window\": window, \"Model\": name, **metrics})\n",
        "        all_probs_for_plots[window].append({\"name\":name, \"y_true\":y_test, \"y_prob\":y_prob})\n",
        "\n",
        "        # training curves\n",
        "        if window == 100:\n",
        "            plt.figure(figsize=(10,4))\n",
        "            plt.plot(history.history['accuracy'], label='Train acc')\n",
        "            plt.plot(history.history['val_accuracy'], label='Val acc')\n",
        "            plt.title(f\"{name} Accuracy (window=100)\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure(figsize=(10,4))\n",
        "            plt.plot(history.history['loss'], label='Train loss')\n",
        "            plt.plot(history.history['val_loss'], label='Val loss')\n",
        "            plt.title(f\"{name} Loss (window=100)\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "# Table A\n",
        "df_A = pd.DataFrame(all_results_A)\n",
        "df_A = df_A.sort_values([\"Window\", \"F1\"], ascending=[True, False])\n",
        "display(df_A)\n",
        "\n",
        "# ROC/PR plots per window\n",
        "for window in WINDOW_SIZES:\n",
        "    plot_roc_pr_curves(all_probs_for_plots[window], title_prefix=f\"(Window={window})\")"
      ],
      "metadata": {
        "id": "rYtn0ChJpxfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UCI external test for window=100 and window=200 (Table B)"
      ],
      "metadata": {
        "id": "_CsSFo0UeKLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_uci_promoters(path):\n",
        "    sequences, labels = [], []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            first = line[0]\n",
        "            if first == '+':\n",
        "                y = 1\n",
        "            elif first == '-':\n",
        "                y = 0\n",
        "            else:\n",
        "                parts = line.split()\n",
        "                if parts and parts[0] in ['+','-']:\n",
        "                    y = 1 if parts[0] == '+' else 0\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            seq = line.split()[-1].replace('.', '').upper()\n",
        "            seq = re.sub(r'[^ATCG]', '', seq)\n",
        "\n",
        "            if len(seq) > 0:\n",
        "                sequences.append(seq)\n",
        "                labels.append(y)\n",
        "\n",
        "    uci_df = pd.DataFrame({\"sequence\": sequences, \"label\": labels})\n",
        "    return uci_df\n",
        "\n",
        "uci_df = load_uci_promoters(\"x/uci.data\")  # update this path !!!\n",
        "\n",
        "\n",
        "all_results_B = []\n",
        "\n",
        "for window in WINDOW_SIZES:\n",
        "    # encoding UCI for this window\n",
        "    X_uci, y_uci = make_X_y_from_df(uci_df, window)\n",
        "\n",
        "    # rebuildng train/test on RegulonDB for this window (so models match window)\n",
        "    Xw, yw = make_X_y_from_df(balanced_df, window)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        Xw, yw, test_size=0.2, stratify=yw, random_state=42\n",
        "    )\n",
        "\n",
        "    # XGBoost\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        eval_metric='logloss',\n",
        "        n_estimators=300,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9\n",
        "    )\n",
        "    xgb_model.fit(X_train.reshape(len(X_train), -1), y_train)\n",
        "    uci_prob = xgb_model.predict_proba(X_uci.reshape(len(X_uci), -1))[:,1]\n",
        "    m = eval_probs(y_uci, uci_prob, threshold=0.5)\n",
        "    all_results_B.append({\"Window\": window, \"Model\":\"XGBoost\", **m})\n",
        "\n",
        "    # Deep models\n",
        "    for name, builder in [(\"CNN\", build_cnn), (\"BiLSTM\", build_bilstm), (\"CNN+BiLSTM\", build_cnn_bilstm)]:\n",
        "        model = builder(window)\n",
        "        es = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "        model.fit(X_train, y_train, validation_split=0.2, epochs=15, batch_size=32, callbacks=[es], verbose=0)\n",
        "\n",
        "        uci_prob = model.predict(X_uci, batch_size=512, verbose=0).ravel()\n",
        "        m = eval_probs(y_uci, uci_prob, threshold=0.5)\n",
        "        all_results_B.append({\"Window\": window, \"Model\": name, **m})\n",
        "\n",
        "df_B = pd.DataFrame(all_results_B).sort_values([\"Window\", \"F1\"], ascending=[True, False])\n",
        "display(df_B)"
      ],
      "metadata": {
        "id": "2k25aNu0p2SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNABERT model"
      ],
      "metadata": {
        "id": "y_XKb9JDfIsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"zhihan1996/DNA_bert_6\"\n",
        "\n",
        "def clean_seq(seq: str) -> str:\n",
        "    return re.sub(r'[^ATCG]', '', str(seq).upper())\n",
        "\n",
        "def normalize_len(seq: str, window: int) -> str:\n",
        "    seq = clean_seq(seq)\n",
        "    if len(seq) >= window:\n",
        "        return seq[:window]\n",
        "    pad = \"\".join(random.choice(\"ATCG\") for _ in range(window - len(seq)))\n",
        "    return seq + pad\n",
        "\n",
        "def seq_to_kmers(seq: str, k: int = 6) -> str:\n",
        "    seq = clean_seq(seq)\n",
        "    if len(seq) < k:\n",
        "        return seq\n",
        "    return \" \".join(seq[i:i+k] for i in range(len(seq) - k + 1))\n",
        "\n",
        "def safe_auc(y_true, y_score):\n",
        "    y_true = np.asarray(y_true)\n",
        "    if len(np.unique(y_true)) < 2:\n",
        "        return float(\"nan\")\n",
        "    return roc_auc_score(y_true, y_score)\n",
        "\n",
        "def safe_prauc(y_true, y_score):\n",
        "    y_true = np.asarray(y_true)\n",
        "    if len(np.unique(y_true)) < 2:\n",
        "        return float(\"nan\")\n",
        "    return average_precision_score(y_true, y_score)\n",
        "\n",
        "def eval_probs_dnabert(y_true, probs, threshold=0.5):\n",
        "    y_true = np.asarray(y_true).astype(int)\n",
        "    probs = np.asarray(probs).astype(float)\n",
        "    preds = (probs >= threshold).astype(int)\n",
        "\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_true, preds),\n",
        "        \"Precision\": precision_score(y_true, preds, zero_division=0),\n",
        "        \"Recall\": recall_score(y_true, preds, zero_division=0),\n",
        "        \"F1\": f1_score(y_true, preds, zero_division=0),\n",
        "        \"ROC_AUC\": safe_auc(y_true, probs),\n",
        "        \"PR_AUC\": safe_prauc(y_true, probs),\n",
        "    }\n",
        "\n",
        "def run_dnABERT_on_balanced_df(\n",
        "    balanced_df: pd.DataFrame,\n",
        "    window: int = 100,\n",
        "    train_size: int = 4000,\n",
        "    eval_size: int = 1000,\n",
        "    epochs: int = 2,\n",
        "    kmer: int = 6,\n",
        "    seed: int = 42\n",
        "):\n",
        "    df_local = balanced_df[[\"sequence\", \"label\"]].copy()\n",
        "    df_local = df_local.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "    total_needed = train_size + eval_size\n",
        "    if len(df_local) < total_needed:\n",
        "        raise ValueError(f\"Not enough samples in balanced_df ({len(df_local)}) for train_size+eval_size={total_needed}\")\n",
        "\n",
        "    df_train = df_local.iloc[:train_size].copy()\n",
        "    df_eval  = df_local.iloc[train_size:train_size+eval_size].copy()\n",
        "\n",
        "    df_train[\"sequence\"] = df_train[\"sequence\"].apply(lambda s: normalize_len(s, window))\n",
        "    df_eval[\"sequence\"]  = df_eval[\"sequence\"].apply(lambda s: normalize_len(s, window))\n",
        "\n",
        "    df_train[\"text\"] = df_train[\"sequence\"].apply(lambda s: seq_to_kmers(s, kmer))\n",
        "    df_eval[\"text\"]  = df_eval[\"sequence\"].apply(lambda s: seq_to_kmers(s, kmer))\n",
        "\n",
        "    train_ds = Dataset.from_pandas(df_train[[\"text\", \"label\"]]).rename_column(\"label\", \"labels\")\n",
        "    eval_ds  = Dataset.from_pandas(df_eval[[\"text\", \"label\"]]).rename_column(\"label\", \"labels\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=2)\n",
        "\n",
        "    def tok(batch):\n",
        "        return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    train_ds = train_ds.map(tok, batched=True)\n",
        "    eval_ds  = eval_ds.map(tok, batched=True)\n",
        "\n",
        "    train_ds = train_ds.remove_columns([\"text\"])\n",
        "    eval_ds  = eval_ds.remove_columns([\"text\"])\n",
        "    train_ds.set_format(\"torch\")\n",
        "    eval_ds.set_format(\"torch\")\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        out = {\n",
        "            \"accuracy\": accuracy_score(labels, preds),\n",
        "            \"precision\": precision_score(labels, preds, zero_division=0),\n",
        "            \"recall\": recall_score(labels, preds, zero_division=0),\n",
        "            \"f1\": f1_score(labels, preds, zero_division=0),\n",
        "        }\n",
        "        out[\"roc_auc\"] = safe_auc(labels, probs)\n",
        "        out[\"pr_auc\"] = safe_prauc(labels, probs)\n",
        "        return out\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=\"x/dnabert_out\", # update this path !!!\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        save_strategy=\"no\",\n",
        "        logging_steps=50,\n",
        "        report_to=\"none\",\n",
        "        seed=seed,\n",
        "        disable_tqdm=True,\n",
        "        logging_strategy=\"no\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=eval_ds,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    pred = trainer.predict(eval_ds)\n",
        "    probs = torch.softmax(torch.tensor(pred.predictions), dim=1)[:, 1].numpy()\n",
        "    labels = pred.label_ids.astype(int)\n",
        "\n",
        "    metrics = eval_probs_dnabert(labels, probs, threshold=0.5)\n",
        "    return model, tokenizer, metrics"
      ],
      "metadata": {
        "id": "2tfX5XjbfIh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNABERT evaluation on UCI"
      ],
      "metadata": {
        "id": "xwy4UiQ5xUPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dnabert_eval_on_uci(model, tokenizer, uci_df: pd.DataFrame, window: int = 100, kmer: int = 6):\n",
        "    df = uci_df.copy()\n",
        "    df[\"sequence\"] = df[\"sequence\"].apply(lambda s: normalize_len(s, window))\n",
        "    texts = [seq_to_kmers(s, kmer) for s in df[\"sequence\"].tolist()]\n",
        "    y_true = df[\"label\"].values.astype(int)\n",
        "\n",
        "    enc = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**enc).logits\n",
        "        probs = torch.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()\n",
        "\n",
        "    return eval_probs_dnabert(y_true, probs, threshold=0.5)"
      ],
      "metadata": {
        "id": "hTq7ZuyJvjy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genome scanning with DNABERT"
      ],
      "metadata": {
        "id": "IsNn4TEHxWrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dnabert_scan_genome_fasta(model, tokenizer, fasta_path: str, window: int = 100, step: int = 10, kmer: int = 6,\n",
        "                             chunk_size: int = 5000, batch_size: int = 16):\n",
        "    from Bio import SeqIO\n",
        "    record = SeqIO.read(fasta_path, \"fasta\")\n",
        "    genome = clean_seq(str(record.seq))\n",
        "    positions = list(range(0, len(genome) - window + 1, step))\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_probs = []\n",
        "    for start in range(0, len(positions), chunk_size):\n",
        "        chunk_pos = positions[start:start+chunk_size]\n",
        "        seqs = [genome[p:p+window] for p in chunk_pos]\n",
        "        texts = [seq_to_kmers(s, kmer) for s in seqs]\n",
        "\n",
        "        enc = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "        chunk_probs = []\n",
        "        with torch.no_grad():\n",
        "            for b in range(0, len(chunk_pos), batch_size):\n",
        "                batch = {k: v[b:b+batch_size] for k, v in enc.items()}\n",
        "                logits = model(**batch).logits\n",
        "                probs = torch.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()\n",
        "                chunk_probs.extend(probs.tolist())\n",
        "\n",
        "        all_probs.extend(chunk_probs)\n",
        "\n",
        "    return positions, np.array(all_probs)"
      ],
      "metadata": {
        "id": "I5lJxWM8vjrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dnabert_model_100, dnabert_tok_100, dnabert_A100 = run_dnABERT_on_balanced_df(\n",
        "    balanced_df=balanced_df,\n",
        "    window=100,\n",
        "    train_size=len(balanced_df) * 0.8,\n",
        "    eval_size=len(balanced_df) * 0.2,\n",
        "    epochs=2\n",
        ")\n",
        "\n",
        "dnabert_model_200, dnabert_tok_200, dnabert_A200 = run_dnABERT_on_balanced_df(\n",
        "    balanced_df=balanced_df,\n",
        "    window=200,\n",
        "    train_size=len(balanced_df) * 0.8,\n",
        "    eval_size=len(balanced_df) * 0.2,\n",
        "    epochs=2\n",
        ")\n",
        "\n",
        "dnabert_B100 = dnabert_eval_on_uci(dnabert_model_100, dnabert_tok_100, uci_df, window=100)\n",
        "dnabert_B200 = dnabert_eval_on_uci(dnabert_model_200, dnabert_tok_200, uci_df, window=200)"
      ],
      "metadata": {
        "id": "9p2bPoGwgLNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Genome scanning"
      ],
      "metadata": {
        "id": "92LN2h49ehjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNABERT is executed locally, check out the scripts folder."
      ],
      "metadata": {
        "id": "pqTg1mVgGZtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genome scanning with BiLSTM"
      ],
      "metadata": {
        "id": "RpmFzYZfyJ3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "record = SeqIO.read(FASTA_PATH, \"fasta\")\n",
        "genome = str(record.seq).upper()\n",
        "genome_len = len(genome)\n",
        "\n",
        "# centered one-hot encoding for genome scanning\n",
        "def batch_one_hot_encode_genome_centered(genome, centers, window):\n",
        "    half = window // 2\n",
        "    arr = np.zeros((len(centers), window, 4), dtype=np.float32)\n",
        "    mapping = {'A':[1,0,0,0], 'T':[0,1,0,0], 'G':[0,0,1,0], 'C':[0,0,0,1]}\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        start = c - half\n",
        "        end = start + window\n",
        "        if start < 0 or end > len(genome):\n",
        "            continue\n",
        "        seq = genome[start:end]\n",
        "        for j, base in enumerate(seq):\n",
        "            if base in mapping:\n",
        "                arr[i, j] = mapping[base]\n",
        "    return arr\n",
        "\n",
        "# chunked prediction for genome scanning\n",
        "def predict_in_chunks_tf_centered(model, genome, centers, window, chunk_size=50000, batch_size=512):\n",
        "    probs_all = []\n",
        "    centers = list(centers)\n",
        "    total = len(centers)\n",
        "\n",
        "    for start in range(0, total, chunk_size):\n",
        "        end = min(start + chunk_size, total)\n",
        "        chunk_centers = centers[start:end]\n",
        "        X_chunk = batch_one_hot_encode_genome_centered(genome, chunk_centers, window)\n",
        "        probs_chunk = model.predict(X_chunk, batch_size=batch_size, verbose=0).ravel()\n",
        "        probs_all.extend(probs_chunk)\n",
        "\n",
        "    return np.array(probs_all), centers\n",
        "\n",
        "# known promoter positions from RegulonDB\n",
        "known_positions = promoter_df[\"posTSS\"].dropna().astype(int).tolist()\n",
        "known_positions = [p for p in known_positions if 0 <= p < genome_len]\n",
        "\n",
        "# training BiLSTM models for genome scanning\n",
        "def train_bilstm_for_scanning(window):\n",
        "    Xw, yw = make_X_y_from_df(balanced_df, window)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        Xw, yw, test_size=0.2, stratify=yw, random_state=42\n",
        "    )\n",
        "\n",
        "    model = build_bilstm(window)\n",
        "    es = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "    model.fit(X_train, y_train, validation_split=0.2, epochs=15,\n",
        "              batch_size=32, callbacks=[es], verbose=0)\n",
        "    return model\n",
        "\n",
        "model_100 = train_bilstm_for_scanning(100)\n",
        "model_200 = train_bilstm_for_scanning(200)\n",
        "\n",
        "\n",
        "combos = [(100, 10), (100, 20), (200, 10), (200, 20)]\n",
        "scan_rows = []\n",
        "\n",
        "for window, step in combos:\n",
        "    model_use = model_100 if window == 100 else model_200\n",
        "    half = window // 2\n",
        "    centers = range(half, genome_len - half, step)\n",
        "\n",
        "    probs, centers_list = predict_in_chunks_tf_centered(\n",
        "        model_use, genome, centers, window\n",
        "    )\n",
        "\n",
        "    thresholds = [0.4, 0.6, 0.8, 0.9] if window == 100 else [0.45]\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        predicted_sites = [centers_list[i] for i, v in enumerate(probs) if v >= threshold]\n",
        "\n",
        "        # merging nearby peaks\n",
        "        merged = []\n",
        "        last = -10**9\n",
        "        for p in sorted(predicted_sites):\n",
        "            if p - last > 200:\n",
        "                merged.append(p)\n",
        "                last = p\n",
        "\n",
        "        # evaluating against known TSS (+-100 bp tolerance)\n",
        "        tol = 100\n",
        "        tp = sum(any(abs(k - p) <= tol for p in merged) for k in known_positions)\n",
        "        fp = len(merged) - tp\n",
        "        fn = len(known_positions) - tp\n",
        "\n",
        "        precision = tp / (tp + fp + 1e-8)\n",
        "        recall = tp / (tp + fn + 1e-8)\n",
        "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "\n",
        "        scan_rows.append({\n",
        "            \"Window\": window,\n",
        "            \"Step\": step,\n",
        "            \"Threshold\": threshold,\n",
        "            \"MergedPeaks\": len(merged),\n",
        "            \"TP\": tp,\n",
        "            \"FP\": fp,\n",
        "            \"FN\": fn,\n",
        "            \"Precision\": precision,\n",
        "            \"Recall\": recall,\n",
        "            \"F1\": f1\n",
        "        })\n",
        "\n",
        "df_scan = pd.DataFrame(scan_rows).sort_values([\"Window\", \"Step\", \"Threshold\"])\n",
        "display(df_scan)"
      ],
      "metadata": {
        "id": "jgD84QbGyJgW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}